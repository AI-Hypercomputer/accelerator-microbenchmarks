benchmarks:
- benchmark_name: "naive_attention"
  benchmark_sweep_params:
  - {batch_range: {start: 1, end: 4, multiplier: 2}, seq_len: 512, d_model: 1024, num_heads: 1, causal: true, scale: false}
  - {batch: 2, seq_len_range: {start: 128, end: 2048, multiplier: 2}, d_model: 1024, num_heads: 1, causal: true, scale: false}
  - {batch: 2, seq_len: 512, d_model_range: {start: 128, end: 4096, multiplier: 2}, num_heads: 1, causal: true, scale: false}
  - {batch: 2, seq_len: 512, d_model: 2048, num_heads_range: {start: 1, end: 16, multiplier: 2}, causal: true, scale: false}
  - {batch: 2, seq_len: 512, d_model: 1024, num_heads: 1, causal: false, scale: false}
  - {batch: 2, seq_len: 512, d_model: 1024, num_heads: 1, causal: true, scale: true}
  trace_dir: "/tmp/microbenchmarks/attention"
  csv_path: "/tmp/microbenchmarks/attention"
  xlml_metrics_dir: "/tmp/microbenchmarks/attention"
- benchmark_name: "pallas_flash_attention"
  benchmark_sweep_params:
  - {batch_range: {start: 1, end: 4, multiplier: 2}, seq_len: 512, d_model: 1024, num_heads: 1, causal: true}
  - {batch: 2, seq_len_range: {start: 128, end: 2048, multiplier: 2}, d_model: 1024, num_heads: 1, causal: true}
  - {batch: 2, seq_len: 512, d_model_range: {start: 128, end: 4096, multiplier: 2}, num_heads: 1, causal: true}
  - {batch: 2, seq_len: 512, d_model: 2048, num_heads_range: {start: 1, end: 16, multiplier: 2}, causal: true}
  - {batch: 2, seq_len: 512, d_model: 1024, num_heads: 1, causal: false}
  trace_dir: "/tmp/microbenchmarks/attention"
  csv_path: "/tmp/microbenchmarks/attention"
  xlml_metrics_dir: "/tmp/microbenchmarks/attention"
- benchmark_name: "splash_attention"
  benchmark_sweep_params:
  - {batch_range: {start: 1, end: 4, multiplier: 2}, seq_len: 512, d_model: 1024, num_heads: 1, causal: true}
  - {batch: 2, seq_len_range: {start: 128, end: 2048, multiplier: 2}, d_model: 1024, num_heads: 1, causal: true}
  - {batch: 2, seq_len: 512, d_model_range: {start: 128, end: 4096, multiplier: 2}, num_heads: 1, causal: true}
  - {batch: 2, seq_len: 512, d_model: 2048, num_heads_range: {start: 1, end: 16, multiplier: 2}, causal: true}
  - {batch: 2, seq_len: 512, d_model: 1024, num_heads: 1, causal: false}
  trace_dir: "/tmp/microbenchmarks/attention"
  csv_path: "/tmp/microbenchmarks/attention"
  xlml_metrics_dir: "/tmp/microbenchmarks/attention"
- benchmark_name: "flax_nnx_attention"
  benchmark_sweep_params:
  - {batch_range: {start: 1, end: 4, multiplier: 2}, seq_len: 512, d_model: 1024, num_heads: 1}
  - {batch: 2, seq_len_range: {start: 128, end: 2048, multiplier: 2}, d_model: 1024, num_heads: 1}
  - {batch: 2, seq_len: 512, d_model_range: {start: 128, end: 4096, multiplier: 2}, num_heads: 1}
  - {batch: 2, seq_len: 512, d_model: 2048, num_heads_range: {start: 1, end: 16, multiplier: 2}}
  trace_dir: "/tmp/microbenchmarks/attention"
  csv_path: "/tmp/microbenchmarks/attention"
  xlml_metrics_dir: "/tmp/microbenchmarks/attention"
- benchmark_name: "flax_linen_attention"
  benchmark_sweep_params:
  - {batch_range: {start: 1, end: 4, multiplier: 2}, seq_len: 512, d_model: 1024, num_heads: 1}
  - {batch: 2, seq_len_range: {start: 128, end: 2048, multiplier: 2}, d_model: 1024, num_heads: 1}
  - {batch: 2, seq_len: 512, d_model_range: {start: 128, end: 4096, multiplier: 2}, num_heads: 1}
  - {batch: 2, seq_len: 512, d_model: 2048, num_heads_range: {start: 1, end: 16, multiplier: 2}}
  trace_dir: "/tmp/microbenchmarks/attention"
  csv_path: "/tmp/microbenchmarks/attention"
  xlml_metrics_dir: "/tmp/microbenchmarks/attention"
- benchmark_name: "keras_attention"
  benchmark_sweep_params:
  - {batch_range: {start: 1, end: 4, multiplier: 2}, seq_len: 512, d_model: 1024, num_heads: 1}
  - {batch: 2, seq_len_range: {start: 128, end: 2048, multiplier: 2}, d_model: 1024, num_heads: 1}
  - {batch: 2, seq_len: 512, d_model_range: {start: 128, end: 4096, multiplier: 2}, num_heads: 1}
  - {batch: 2, seq_len: 512, d_model: 2048, num_heads_range: {start: 1, end: 16, multiplier: 2}}
  trace_dir: "/tmp/microbenchmarks/attention"
  csv_path: "/tmp/microbenchmarks/attention"
  xlml_metrics_dir: "/tmp/microbenchmarks/attention"
- benchmark_name: "tokamax_splash_attention"
  benchmark_sweep_params:
  - {batch_size_range: {start: 1, end: 8, multiplier: 2}, q_seq_len: 1024, kv_seq_len: 1024, q_heads: 8, kv_heads: 8, qk_head_dim: 128, v_head_dim: 128, mode_list: ["fwd", "bwd"], causal: true, num_samples: 256, tune_pallas_only: true}
  - {batch_size: 2, q_seq_len_range: {start: 512, end: 4096, multiplier: 4}, kv_seq_len_range: {start: 512, end: 4096, multiplier: 4}, q_heads: 8, kv_heads: 8, qk_head_dim: 128, v_head_dim: 128, mode: "fwd", causal_list: [true, false], num_samples: 256, tune_pallas_only: true}
  - {batch_size: 2, q_seq_len: 1024, kv_seq_len: 1024, q_heads: 16, kv_heads_list: [1, 4, 16], qk_head_dim: 128, v_head_dim: 128, mode_list: ["fwd", "bwd"], causal: true, num_samples: 256, tune_pallas_only: true}
  - {batch_size: 2, q_seq_len: 1024, kv_seq_len: 1024, q_heads: 8, kv_heads: 8, qk_head_dim_list: [64, 128, 256], v_head_dim_list: [64, 128, 256], mode: "fwd", causal: true, num_samples: 256, tune_pallas_only: true}
  - {batch_size: 2, q_seq_len: 1024, kv_seq_len: 4096, q_heads: 8, kv_heads: 8, qk_head_dim: 128, v_head_dim: 128, mode_list: ["fwd", "bwd"], causal: false, num_samples: 256, tune_pallas_only: true}
  - {batch_size: 1, q_seq_len: 8192, kv_seq_len: 8192, q_heads: 16, kv_heads: 16, qk_head_dim: 256, v_head_dim: 256, mode: "combined", causal: true, num_samples: 1024, tune_pallas_only: true}
  trace_dir: "/tmp/microbenchmarks/attention"
  csv_path: "/tmp/microbenchmarks/attention"
  xlml_metrics_dir: "/tmp/microbenchmarks/attention"
 